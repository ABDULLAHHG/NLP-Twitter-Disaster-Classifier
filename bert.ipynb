{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e06c541-35c8-4b05-9e80-efc22af6b958",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d743bf-ba1b-45c7-8e68-52a320c7ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72df25b-bbc5-46b4-8882-24e14051bc88",
   "metadata": {},
   "source": [
    "### load the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84014f1f-6508-4472-b47d-1b54c7b4c831",
   "metadata": {},
   "source": [
    "##### without change the dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de930d65-b4e2-4b73-846a-cfc67158166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34f3a73-f4f2-481b-a960-4b3658fd27d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape = (7613, 5) \n",
      "Training Set Memory Usage = 0.29 MB\n",
      "Test Set Shape  = (3263, 4) \n",
      "Test Set Memory Usage = 0.10 MB\n"
     ]
    }
   ],
   "source": [
    "# for training dataset \n",
    "print(\"Training Set Shape = {} \".format(df_train.shape))\n",
    "print(\"Training Set Memory Usage = {:.2f} MB\".format(df_train.memory_usage().sum() / 1024**2))\n",
    "\n",
    "# for test dataset \n",
    "print(\"Test Set Shape  = {} \".format(df_test.shape))\n",
    "print(\"Test Set Memory Usage = {:.2f} MB\".format(df_test.memory_usage().sum() / 1024**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e8986-7c44-4b35-94c0-e777bf967d45",
   "metadata": {},
   "source": [
    "##### with change the dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf0383f-d4bd-4318-99fa-cffacd8904ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\" , dtype = {\"id\":np.int16 , \"target\" : np.int8})\n",
    "df_test = pd.read_csv(\"test.csv\" , dtype = {\"id\" : np.int16} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db841082-7d76-4419-9706-f98080d4ac08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape = (7613, 5) \n",
      "Training Set Memory Usage = 0.20 MB\n",
      "Test Set Shape  = (3263, 4) \n",
      "Test Set Memory Usage = 0.08 MB\n"
     ]
    }
   ],
   "source": [
    "# for training dataset \n",
    "print(\"Training Set Shape = {} \".format(df_train.shape))\n",
    "print(\"Training Set Memory Usage = {:.2f} MB\".format(df_train.memory_usage().sum() / 1024**2))\n",
    "\n",
    "# for test dataset \n",
    "print(\"Test Set Shape  = {} \".format(df_test.shape))\n",
    "print(\"Test Set Memory Usage = {:.2f} MB\".format(df_test.memory_usage().sum() / 1024**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee83365b-41e5-4907-81be-c36a3075e038",
   "metadata": {},
   "source": [
    "##### data cleaning \n",
    "* we will drop all nan values and limit the row in training dataset to 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02b69bd-3e6b-4dc9-a9f2-be0486280ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ff6f08-8c98-42b6-8332-ab9bf56d0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.iloc[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "342ad114-a212-4a74-ad00-5c2557c28d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape = (1000, 5) \n",
      "Training Set Memory Usage = 0.03 MB\n"
     ]
    }
   ],
   "source": [
    "# for training dataset \n",
    "print(\"Training Set Shape = {} \".format(df_train.shape))\n",
    "print(\"Training Set Memory Usage = {:.2f} MB\".format(df_train.memory_usage().sum() / 1024**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8341a1b6-ca6f-4442-bcee-5899d2e88242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    607\n",
       "1    393\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0d6c7c-4a9e-4062-b856-4805817a05eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London</td>\n",
       "      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Niall's place | SAF 12 SQUAD |</td>\n",
       "      <td>@sunkxssedharry will you wear shorts for race ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NIGERIA</td>\n",
       "      <td>#PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Los Angeles, Califnordia</td>\n",
       "      <td>PSA: IÛªm splitting my personalities.\\r\\n\\r\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>10804</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Love Reiss</td>\n",
       "      <td>@yakubOObs think he deactivated because his no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>10806</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Seattle Washington</td>\n",
       "      <td>RT CNBC '3 words from Disney CEO Bob Iger wrec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>10807</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Acey mountain islanddåÇTorontoåÈ</td>\n",
       "      <td>Smackdown tyme this should put me in a good mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>10816</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>los angeles</td>\n",
       "      <td>@thrillhho jsyk I haven't stopped thinking abt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>10820</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Brussels, Belgium</td>\n",
       "      <td>@stighefootball Begovic has been garbage. He g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2158 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                          location  \\\n",
       "15       46   ablaze                            London   \n",
       "16       47   ablaze    Niall's place | SAF 12 SQUAD |   \n",
       "17       51   ablaze                           NIGERIA   \n",
       "18       58   ablaze                    Live On Webcam   \n",
       "19       60   ablaze          Los Angeles, Califnordia   \n",
       "...     ...      ...                               ...   \n",
       "3246  10804  wrecked                        Love Reiss   \n",
       "3247  10806  wrecked                Seattle Washington   \n",
       "3248  10807  wrecked  Acey mountain islanddåÇTorontoåÈ   \n",
       "3249  10816  wrecked                       los angeles   \n",
       "3250  10820  wrecked                 Brussels, Belgium   \n",
       "\n",
       "                                                   text  \n",
       "15    Birmingham Wholesale Market is ablaze BBC News...  \n",
       "16    @sunkxssedharry will you wear shorts for race ...  \n",
       "17    #PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...  \n",
       "18    Check these out: http://t.co/rOI2NSmEJJ http:/...  \n",
       "19    PSA: IÛªm splitting my personalities.\\r\\n\\r\\n...  \n",
       "...                                                 ...  \n",
       "3246  @yakubOObs think he deactivated because his no...  \n",
       "3247  RT CNBC '3 words from Disney CEO Bob Iger wrec...  \n",
       "3248  Smackdown tyme this should put me in a good mo...  \n",
       "3249  @thrillhho jsyk I haven't stopped thinking abt...  \n",
       "3250  @stighefootball Begovic has been garbage. He g...  \n",
       "\n",
       "[2158 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e81236fb-93ec-4f28-b9ee-fd1bfda151c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_train[\"text\"]\n",
    "labels = df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327bcffd-4dfc-4421-a33f-1c52b827fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer , BertForSequenceClassification \n",
    "from torch.utils.data import DataLoader , Dataset \n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719eced9-17b1-4a60-b975-8de50c6d879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "774dbf9b-c72b-415f-adc5-4d81f817cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self , texts , labels , tokenizer , max_len):\n",
    "        self.texts = texts.reset_index(drop = True)\n",
    "        self.labels = labels.reset_index(drop = True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding = \"max_length\",\n",
    "            truncation = True,\n",
    "            return_tensors = \"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\" : encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\" : encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\" : torch.tensor(label , dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e226343d-2061-40b9-a301-d99725b67ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_len = 256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7a3af46-dd81-47b6-b0d6-84978bc8d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "train_texts , val_texts , train_labels , val_labels = train_test_split(text , labels , random_state = 42 , stratify = df_train[\"target\"])\n",
    "\n",
    "# Creating dataloader instance \n",
    "train_dataset = TextDataset(train_texts , train_labels , tokenizer , max_len)\n",
    "val_dataset = TextDataset(val_texts , val_labels , tokenizer , max_len)\n",
    "\n",
    "# Creating loader object \n",
    "train_loader = DataLoader(train_dataset , batch_size = 16 , shuffle = True)\n",
    "val_loader = DataLoader(val_dataset , batch_size = 16 , shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c953e440-3d9b-46ea-a7c1-c240a457235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\New folder\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initializing Bert Model \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\" , num_labels = 2)\n",
    "optimizer = AdamW(model.parameters() , lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92fcc5f0-69b8-4438-9cc4-6cff752e978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def train_epoch(model,data_loader,optimizer,device):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids=batch['input_ids'].to(device)\n",
    "        attention_mask=batch['attention_mask'].to(device)\n",
    "        labels=batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(input_ids,attention_mask=attention_mask,labels=labels)\n",
    "        loss=outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "        \n",
    "    return total_loss/len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return accuracy_score(true_labels, preds)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "358354f5-801d-4204-93d8-51a5c59ca576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\New folder\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 , Loss: 0.5901380542744982, Validation Accuracy:0.808\n",
      "Epoch 2 , Loss: 0.39681873771738496, Validation Accuracy:0.776\n",
      "Epoch 3 , Loss: 0.29691163307808816, Validation Accuracy:0.804\n",
      "Epoch 4 , Loss: 0.17879277523210707, Validation Accuracy:0.844\n",
      "Epoch 5 , Loss: 0.10560117455873083, Validation Accuracy:0.828\n",
      "Epoch 6 , Loss: 0.06969476727015794, Validation Accuracy:0.784\n",
      "Epoch 7 , Loss: 0.05836831020349835, Validation Accuracy:0.84\n",
      "Epoch 8 , Loss: 0.06642380566831599, Validation Accuracy:0.772\n",
      "Epoch 9 , Loss: 0.03355034736004916, Validation Accuracy:0.816\n",
      "Epoch 10 , Loss: 0.0247619683913728, Validation Accuracy:0.836\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model , train_loader , optimizer , device)\n",
    "    val_accuracy = evaluate(model , val_loader , device)\n",
    "    print(f\"Epoch {epoch + 1} , Loss: {train_loss}, Validation Accuracy:{val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2a410-b189-45e2-9e32-46e5073031e7",
   "metadata": {},
   "source": [
    "* `Validation Accuracy:0.836` not bad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57f99157-f0be-4f44-ba35-08e8831496a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6871f-4599-4cf7-a7a5-945e21137e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
