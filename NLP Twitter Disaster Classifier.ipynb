{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a937c9-c582-44ac-b956-920c8c6100be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615d62b9-c7c7-4d69-8cdb-08deaea6c189",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a28e2-f5b9-49a8-be44-bfdf0d5da80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_target = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e492ff5-6ff5-4c23-b9c2-46282cbb7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe20439-2d72-4776-ba90-6d84bd23708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deec38e-f9fa-41d9-9312-3d9c8e221176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d6a08-26ea-49e2-8177-995298b28253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.keyword = df_train.keyword.fillna(df_train.keyword.mode()[0])\n",
    "df_test.keyword = df_test.keyword.fillna(df_test.keyword.mode()[0])\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5accf53-f361-4985-88bf-30f3de47bc7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b5bcd-e280-49d2-bde9-87d1ea05ea19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c9f64c-032d-4a78-8f1c-6e2b9e5b6521",
   "metadata": {},
   "source": [
    "###  Text Clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f701f4e-691d-4bd4-8266-160701c1de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and packages for text (pre-)processing \n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a36e8-6ea9-499b-ad1d-034ff148805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"clean_text\"] = df_train[\"text\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94a001-2825-430e-8750-cd443f631ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62b9ae3-37a9-4c96-b152-0566dab9cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7985810-3f64-4d6c-9431-49061ab658cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c270f5c9-5064-426b-83ab-d781c4c05c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"clean_text\"] = df_train[\"clean_text\"].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b76406-03fe-4b8f-a47c-68dd60c50b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd3d3c-faa4-4fb2-bb18-926359972be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train[\"text\"][67])\n",
    "print(df_train[\"clean_text\"][67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089982c4-f8c2-4a3d-9eb2-c5542a93a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    \"\"\"\n",
    "        Remove URLs from a sample string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"\n",
    "        Remove the html in sample text\n",
    "    \"\"\"\n",
    "    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html, \"\", text)\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "        Remove non-ASCII characters \n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\x00-\\x7f]',r'', text) # or ''.join([x for x in text if x in string.printable]) \n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "        Remove the punctuation\n",
    "    \"\"\"\n",
    "#     return re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede2716-9e61-4f0f-8459-ea203a652789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"clean_text\"] = df_train[\"clean_text\"].apply(lambda x: remove_URL(x))\n",
    "\n",
    "df_train[\"clean_text\"] = df_train[\"clean_text\"].apply(lambda x: remove_html(x))\n",
    "\n",
    "df_train[\"clean_text\"] = df_train[\"clean_text\"].apply(lambda x: remove_non_ascii(x))\n",
    "\n",
    "df_train[\"clean_text\"] = df_train[\"clean_text\"].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48758256-51e6-476c-8835-61998363daa6",
   "metadata": {},
   "source": [
    "###  Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389bbfd-3e54-4981-8d64-48601a97d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Tokenizing the tweet base texts.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df_train['tokenized'] = df_train['clean_text'].apply(word_tokenize)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf7619-8a35-42b5-bf99-d4befa0c9973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing stopwords.\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "df_train['stopwords_removed'] = df_train['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc72d3b6-fe41-4c51-afdf-891a8a076fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "def porter_stemmer(text):\n",
    "    \"\"\"\n",
    "        Stem words in list of tokenized words with PorterStemmer\n",
    "    \"\"\"\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems\n",
    "\n",
    "def lancaster_stemmer(text):\n",
    "    \"\"\"\n",
    "        Stem words in list of tokenized words with LancasterStemmer\n",
    "    \"\"\"\n",
    "    stemmer = nltk.LancasterStemmer()\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems\n",
    "\n",
    "def snowball_stemmer(text):\n",
    "    \"\"\"\n",
    "        Stem words in list of tokenized words with SnowballStemmer\n",
    "    \"\"\"\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    stems = [stemmer.stem(i) for i in text]\n",
    "    return stems\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9c336-8d05-4cf8-b1ec-d57dd689a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "df_train['porter_stemmer'] = df_train['stopwords_removed'].apply(lambda x: porter_stemmer(x))\n",
    "\n",
    "df_train['lancaster_stemmer'] = df_train['stopwords_removed'].apply(lambda x: lancaster_stemmer(x))\n",
    "\n",
    "df_train['snowball_stemmer'] = df_train['stopwords_removed'].apply(lambda x: snowball_stemmer(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7918efd7-cf4a-4faf-83b2-dd0bd60d181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomments if u havent download it yet\n",
    "# nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
    "    \"\"\"\n",
    "        Create pos_tag with wordnet format\n",
    "    \"\"\"\n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    # map the pos tagging output with wordnet output \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "    return pos_tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d145c-1d5f-416c-9b2c-c8c5076adbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "df_train['combined_postag_wnet'] = df_train['stopwords_removed'].apply(lambda x: pos_tag_wordnet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ff3c1-5a9a-4b24-b778-3e7e13a089e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    \"\"\"\n",
    "        Lemmatize the tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8fe6b-5001-4847-ac21-7b449938b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without POS Tagging\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_train['lemmatize_word_wo_pos'] = df_train['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "df_train['lemmatize_word_wo_pos'] = df_train['lemmatize_word_wo_pos'].apply(lambda x: [word for word in x if word not in stop])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef47491-9ed8-4d3a-8a34-f5c12dd007d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "# Test with POS Tagging\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_train['lemmatize_word_w_pos'] = df_train['combined_postag_wnet'].apply(lambda x: lemmatize_word(x))\n",
    "df_train['lemmatize_word_w_pos'] = df_train['lemmatize_word_w_pos'].apply(lambda x: [word for word in x if word not in stop]) # double check to remove stop words\n",
    "df_train['lemmatize_text'] = [' '.join(map(str, l)) for l in df_train['lemmatize_word_w_pos']] # join back to text\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9f2d6-716c-454f-aab7-12015f1616d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(df_train[\"text\"][31])\n",
    "print(df_train[\"combined_postag_wnet\"][31])\n",
    "print(df_train[\"lemmatize_word_wo_pos\"][31])\n",
    "print(df_train[\"lemmatize_word_w_pos\"][31])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0b6769-1964-433e-ba9a-7a79f89d1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31c1f05-047f-4053-80d4-d32ade00dcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519fe9e-be8a-41c0-8145-f8ca07405eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def cv(data, ngram = 1, MAX_NB_WORDS = 75000):\n",
    "    count_vectorizer = CountVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)\n",
    "    emb = count_vectorizer.fit_transform(data).toarray()\n",
    "    print(\"count vectorize with\", str(np.array(emb).shape[1]), \"features\")\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c0fe36-d29a-44c2-9dec-8401590c36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(emb, feat, ngram, compared_sentence=0):\n",
    "    print(ngram,\"bag-of-words: \")\n",
    "    print(feat.get_feature_names(), \"\\n\")\n",
    "    print(ngram,\"bag-of-feature: \")\n",
    "    print(test_cv_1gram.vocabulary_, \"\\n\")\n",
    "    print(\"BoW matrix:\")\n",
    "    print(pd.DataFrame(emb.transpose(), index = feat.get_feature_names()).head(), \"\\n\")\n",
    "    print(ngram,\"vector example:\")\n",
    "    print(df_train[\"lemmatize_text\"][compared_sentence])\n",
    "    print(emb[compared_sentence], \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16397c74-4ddd-4838-957a-1f0ecc61d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = df_train[\"lemmatize_text\"][:5].tolist()\n",
    "print(\"The test corpus: \", test_corpus, \"\\n\")\n",
    "\n",
    "test_cv_em_1gram, test_cv_1gram = cv(test_corpus, ngram=1)\n",
    "print_out(test_cv_em_1gram, test_cv_1gram, ngram=\"Uni-gram\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238882e6-0723-4e3e-b3cf-061d8e79e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360e491d-0716-4afd-9c05-316e39ba8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d8b21-e37c-4c53-8825-77934631675f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57cdd02-6a1a-4efd-8d65-c398ca0fe315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b9f02-7cd7-4f3b-a949-3bae7e8aa38a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
